{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LzwX3RKmJy5w5N40PfqchAxcjPSVN0TD",
      "authorship_tag": "ABX9TyOMfmDFg49qEs4R/IUH7IWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amal-Emad/JODDB/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwDKo6PhROpW",
        "outputId": "ac3f827e-3ddb-45eb-9cdc-04356b912bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.52-py3-none-any.whl (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.9/800.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.0/800.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.52 ultralytics-thop-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #paths\n",
        " PersonImg = '/content/person/PersonImg'\n",
        " images = '/content/person/images'\n",
        " labels = '/content/person/labels'"
      ],
      "metadata": {
        "id": "6SOkDNcuM_Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "from ultralytics import YOLO\n",
        "model = YOLO(\"yolov8n.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSYQIR4VVt0i",
        "outputId": "e658f245-206b-4382-d871-c8ef709e77aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 58.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# person process for crop and label\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "results = model.predict('/content/person/images', classes=[0] , save_txt = True , save_crop=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''os.makedirs(images, exist_ok=True)\n",
        "os.makedirs(labels, exist_ok=True)\n",
        "\n",
        "image_files = [f for f in os.listdir(PersonImg) if os.path.isfile(os.path.join(PersonImg, f))]'''\n",
        "'''for image_file in image_files:   #read image\n",
        "      image_path = os.path.join(PersonImg, image_file)\n",
        "      image = cv2.imread(image_path)\n",
        "      results = model.predict(image_path, classes=[0])\n",
        "\n",
        "\n",
        "      for i, result in enumerate(results):         #cropping and labeling\n",
        "            if result is not None and len(result) > 0:\n",
        "                for j, box in enumerate(result.boxes):\n",
        "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                    crop_img = image[y1:y2, x1:x2]\n",
        "                    crop_filename = f'{os.path.splitext(image_file)[0]}_crop_{i}_{j}.jpg'\n",
        "                    crop_path = os.path.join(PersonImg, crop_filename)\n",
        "                    cv2.imwrite(crop_path, crop_img)\n",
        "\n",
        "                    img_width, img_height = image.shape[1], image.shape[0]\n",
        "                    x_center = (x1 + x2) / 2 / img_width\n",
        "                    y_center = (y1 + y2) / 2 / img_height\n",
        "                    width = (x2 - x1) / img_width\n",
        "                    height = (y2 - y1) / img_height\n",
        "\n",
        "                    class_id = int(box.cls[0])\n",
        "                    label_filename = f'{os.path.splitext(image_file)[0]}_crop_{i}_{j}.txt'\n",
        "                    txt_path = os.path.join(labels, label_filename)\n",
        "\n",
        "                    with open(txt_path, 'w') as f:\n",
        "                        f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "                    print(f'Saved crop: {crop_path}')\n",
        "                    print(f'Saved coordinates: {txt_path}')\n",
        "            else:\n",
        "                print(f\"No detections in image {image_file}\")'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "all0i5XBNUYo",
        "outputId": "13849610-3a44-4860-9faf-bb5fae459f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/11 /content/person/images/czzcx_crop_0_0.jpg: 640x448 1 face, 143.2ms\n",
            "image 2/11 /content/person/images/ddddddddddddd_crop_0_0.jpg: 608x640 1 face, 179.8ms\n",
            "image 3/11 /content/person/images/ddddddddddddd_crop_0_1.jpg: 640x384 (no detections), 121.8ms\n",
            "image 4/11 /content/person/images/fds_crop_0_0.jpg: 640x512 1 face, 155.8ms\n",
            "image 5/11 /content/person/images/fds_crop_0_1.jpg: 640x640 2 faces, 217.8ms\n",
            "image 6/11 /content/person/images/images_crop_0_0.jpg: 640x512 1 face, 190.6ms\n",
            "image 7/11 /content/person/images/qd_crop_0_0.jpg: 640x608 1 face, 208.8ms\n",
            "image 8/11 /content/person/images/qweq_crop_0_0.jpg: 640x256 1 face, 99.4ms\n",
            "image 9/11 /content/person/images/qweq_crop_0_1.jpg: 640x256 1 face, 98.5ms\n",
            "image 10/11 /content/person/images/qweq_crop_0_2.jpg: 640x224 1 face, 93.7ms\n",
            "image 11/11 /content/person/images/qweq_crop_0_3.jpg: 640x256 (no detections), 91.1ms\n",
            "Speed: 3.6ms preprocess, 145.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 256)\n",
            "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
            "9 labels saved to runs/detect/predict2/labels\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for image_file in image_files:   #read image\\n      image_path = os.path.join(PersonImg, image_file)\\n      image = cv2.imread(image_path)\\n      results = model.predict(image_path, classes=[0])\\n      \\n      \\n      for i, result in enumerate(results):         #cropping and labeling\\n            if result is not None and len(result) > 0:\\n                for j, box in enumerate(result.boxes):\\n                    x1, y1, x2, y2 = map(int, box.xyxy[0])\\n                    crop_img = image[y1:y2, x1:x2]\\n                    crop_filename = f\\'{os.path.splitext(image_file)[0]}_crop_{i}_{j}.jpg\\'\\n                    crop_path = os.path.join(PersonImg, crop_filename)\\n                    cv2.imwrite(crop_path, crop_img)\\n                    \\n                    img_width, img_height = image.shape[1], image.shape[0]\\n                    x_center = (x1 + x2) / 2 / img_width\\n                    y_center = (y1 + y2) / 2 / img_height\\n                    width = (x2 - x1) / img_width\\n                    height = (y2 - y1) / img_height\\n                    \\n                    class_id = int(box.cls[0])\\n                    label_filename = f\\'{os.path.splitext(image_file)[0]}_crop_{i}_{j}.txt\\'\\n                    txt_path = os.path.join(labels, label_filename)\\n                    \\n                    with open(txt_path, \\'w\\') as f:\\n                        f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\\n                    \\n                    print(f\\'Saved crop: {crop_path}\\')\\n                    print(f\\'Saved coordinates: {txt_path}\\')\\n            else:\\n                print(f\"No detections in image {image_file}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#paths\n",
        "images = '/content/person/images'\n",
        "faceimg = '/content/faces/faceimg'\n",
        "labelimg = '/content/faces/labelimg'"
      ],
      "metadata": {
        "id": "5rKM88K7Phz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pretrained model\n",
        "face_model = YOLO(\"/content/best.pt\")"
      ],
      "metadata": {
        "id": "fEYUvKufPtpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply model on result of the person model\n",
        "results = face_model(\"/content/person/images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESgwKHXrRQ-O",
        "outputId": "d9a70454-8147-4543-ba7d-b6063ec9a176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/11 /content/person/images/czzcx_crop_0_0.jpg: 640x448 1 face, 182.4ms\n",
            "image 2/11 /content/person/images/ddddddddddddd_crop_0_0.jpg: 608x640 1 face, 190.5ms\n",
            "image 3/11 /content/person/images/ddddddddddddd_crop_0_1.jpg: 640x384 (no detections), 119.7ms\n",
            "image 4/11 /content/person/images/fds_crop_0_0.jpg: 640x512 1 face, 160.2ms\n",
            "image 5/11 /content/person/images/fds_crop_0_1.jpg: 640x640 2 faces, 195.9ms\n",
            "image 6/11 /content/person/images/images_crop_0_0.jpg: 640x512 1 face, 159.6ms\n",
            "image 7/11 /content/person/images/qd_crop_0_0.jpg: 640x608 1 face, 190.9ms\n",
            "image 8/11 /content/person/images/qweq_crop_0_0.jpg: 640x256 1 face, 84.4ms\n",
            "image 9/11 /content/person/images/qweq_crop_0_1.jpg: 640x256 1 face, 84.5ms\n",
            "image 10/11 /content/person/images/qweq_crop_0_2.jpg: 640x224 1 face, 86.1ms\n",
            "image 11/11 /content/person/images/qweq_crop_0_3.jpg: 640x256 (no detections), 85.3ms\n",
            "Speed: 3.0ms preprocess, 140.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process face images\n",
        "image_files = [f for f in os.listdir(images) if os.path.isfile(os.path.join(images, f))]\n",
        "\n",
        "for image_file in image_files:    #read images\n",
        "    image_path = os.path.join(images, image_file)\n",
        "    image = cv2.imread(image_path)\n",
        "    results = face_model.predict(image_path)\n",
        "\n",
        "    for i, result in enumerate(results):       #cropping and labeling\n",
        "        if result is not None and len(result) > 0:\n",
        "            for j, box in enumerate(result.boxes):\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                crop_img = image[y1:y2, x1:x2]\n",
        "                crop_filename = f'{os.path.splitext(image_file)[0]}_facecrop_{i}_{j}.jpg'\n",
        "                crop_path = os.path.join(faceimg, crop_filename)\n",
        "                cv2.imwrite(crop_path, crop_img)\n",
        "\n",
        "                img_width, img_height = image.shape[1], image.shape[0]\n",
        "                x_center = (x1 + x2) / 2 / img_width\n",
        "                y_center = (y1 + y2) / img_height\n",
        "                width = (x2 - x1) / img_width\n",
        "                height = (y2 - y1) / img_height\n",
        "\n",
        "                class_id = int(box.cls[0])\n",
        "                label_filename = f'{os.path.splitext(image_file)[0]}_facecrop_{i}_{j}.txt'\n",
        "                txt_path = os.path.join(labelimg, label_filename)\n",
        "\n",
        "                with open(txt_path, 'w') as f:\n",
        "                    f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "                print(f'saved face crop: {crop_path}')\n",
        "                print(f'saved face coordinated: {txt_path}')\n",
        "        else:\n",
        "            print(f\"No face detections in image {image_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93g2lkj0U5iC",
        "outputId": "6fc5b70f-829e-40d9-99ff-5cdf7a5ccf18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/person/images/qweq_crop_0_1.jpg: 640x256 1 face, 84.4ms\n",
            "Speed: 1.9ms preprocess, 84.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 256)\n",
            "Saved face crop: /content/faces/faceimg/qweq_crop_0_1_facecrop_0_0.jpg\n",
            "Saved face coordinates: /content/faces/labelimg/qweq_crop_0_1_facecrop_0_0.txt\n",
            "\n",
            "image 1/1 /content/person/images/ddddddddddddd_crop_0_1.jpg: 640x384 (no detections), 125.8ms\n",
            "Speed: 2.3ms preprocess, 125.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "No face detections in image ddddddddddddd_crop_0_1.jpg\n",
            "\n",
            "image 1/1 /content/person/images/fds_crop_0_0.jpg: 640x512 1 face, 168.6ms\n",
            "Speed: 4.8ms preprocess, 168.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Saved face crop: /content/faces/faceimg/fds_crop_0_0_facecrop_0_0.jpg\n",
            "Saved face coordinates: /content/faces/labelimg/fds_crop_0_0_facecrop_0_0.txt\n",
            "\n",
            "image 1/1 /content/person/images/ddddddddddddd_crop_0_0.jpg: 608x640 1 face, 202.5ms\n",
            "Speed: 5.5ms preprocess, 202.5ms inference, 0.8ms postprocess per image at shape (1, 3, 608, 640)\n",
            "Saved face crop: /content/faces/faceimg/ddddddddddddd_crop_0_0_facecrop_0_0.jpg\n",
            "Saved face coordinates: /content/faces/labelimg/ddddddddddddd_crop_0_0_facecrop_0_0.txt\n",
            "\n",
            "image 1/1 /content/person/images/czzcx_crop_0_0.jpg: 640x448 1 face, 142.6ms\n",
            "Speed: 4.7ms preprocess, 142.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Saved face crop: /content/faces/faceimg/czzcx_crop_0_0_facecrop_0_0.jpg\n",
            "Saved face coordinates: /content/faces/labelimg/czzcx_crop_0_0_facecrop_0_0.txt\n",
            "\n",
            "image 1/1 /content/person/images/qweq_crop_0_2.jpg: 640x224 1 face, 97.4ms\n",
            "Speed: 3.1ms preprocess, 97.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 224)\n",
            "Saved face crop: /content/faces/faceimg/qweq_crop_0_2_facecrop_0_0.jpg\n",
            "Saved face coordinates: /content/faces/labelimg/qweq_crop_0_2_facecrop_0_0.txt\n",
            "\n",
            "image 1/1 /content/person/images/fds_crop_0_1.jpg: 640x640 2 faces, 206.3ms\n",
            "Speed: 3.7ms preprocess, 206.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Saved face crop: /content/faces/faceimg/fds_crop_0_1_facecrop_0_0.jpg\n",
            "Saved face coordinates: /content/faces/labelimg/fds_crop_0_1_facecrop_0_0.txt\n",
            "Saved face crop: /content/faces/faceimg/fds_crop_0_1_facecrop_0_1.jpg\n",
            "Saved face coordinates: /content/faces/labelimg/fds_crop_0_1_facecrop_0_1.txt\n",
            "\n",
            "image 1/1 /content/person/images/qweq_crop_0_3.jpg: 640x256 (no detections), 90.5ms\n",
            "Speed: 3.3ms preprocess, 90.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 256)\n",
            "No face detections in image qweq_crop_0_3.jpg\n",
            "\n",
            "image 1/1 /content/person/images/qd_crop_0_0.jpg: 640x608 1 face, 195.8ms\n",
            "Speed: 4.2ms preprocess, 195.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 608)\n",
            "Saved face crop: /content/faces/faceimg/qd_crop_0_0_facecrop_0_0.jpg\n",
            "Saved face coordinates: /content/faces/labelimg/qd_crop_0_0_facecrop_0_0.txt\n",
            "\n",
            "image 1/1 /content/person/images/images_crop_0_0.jpg: 640x512 1 face, 175.9ms\n",
            "Speed: 4.9ms preprocess, 175.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Saved face crop: /content/faces/faceimg/images_crop_0_0_facecrop_0_0.jpg\n",
            "Saved face coordinates: /content/faces/labelimg/images_crop_0_0_facecrop_0_0.txt\n",
            "\n",
            "image 1/1 /content/person/images/qweq_crop_0_0.jpg: 640x256 1 face, 84.8ms\n",
            "Speed: 2.8ms preprocess, 84.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 256)\n",
            "Saved face crop: /content/faces/faceimg/qweq_crop_0_0_facecrop_0_0.jpg\n",
            "Saved face coordinates: /content/faces/labelimg/qweq_crop_0_0_facecrop_0_0.txt\n"
          ]
        }
      ]
    }
  ]
}